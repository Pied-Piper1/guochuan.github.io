<!DOCTYPE html>
<html lang="zh">
  <head>
    
    <meta charset="UTF-8">
    <title>Active and Volumetric Stereo - guochuan</title>
    <link rel="shortcut icon" href="/static/img/icon.png">
    <link rel="icon" href="/static/img/icon.png" sizes="192x192"/>
    <link rel="stylesheet" href="/static/todo.css">
    <link rel="stylesheet" href="/static/custom.css">
    
<link rel="stylesheet" href="/static/kico.css">
<link rel="stylesheet" href="/static/hingle.css">

    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <meta property="og:site_name" content="guochuan">
    <meta property="og:title" content="Active and Volumetric Stereo"/>
    
  
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>

  <body>
    <header>
    <div class="head-title">
        <h4>guochuan</h4>
    </div>
    <div class="head-action">
        <div class="toggle-btn"></div>
        <div class="light-btn"></div>
        <div class="search-btn"></div>
    </div>
    <form class="head-search" method="post">
        <input type="text" name="s" placeholder="搜索什么？">
    </form>
    <nav class="head-menu">
        <a href="/">首页</a>
        <div class="has-child">
            <a href>分类</a>
            <div class="sub-menu">
                <a class="category-link" href="/categories/computer-vision/">computer vision</a><a class="category-link" href="/categories/reinforcement-learning/">reinforcement learning</a>
            </div>
        </div>
        
            <a href="/about">关于我</a>
        
            <a href="/friends">朋友们</a>
        
    </nav>
</header>

    <main>
    <div class="wrap min">
        <section class="post-title">
            <h2>Active and Volumetric Stereo</h2>
            <div class="post-meta">
<time class="date">2025.09.03</time>

    <time class="date updated-time" style="margin-left: 10px; color: #999;">
        (Updated: 2025.09.03)
    </time>

            
                <span class="category"><a class="category-link" href="/categories/computer-vision/">computer vision</a></span>
            
            </div>
        </section>
        <article class="post-content">
        
            <blockquote>
<p>Traditionally, the main idea of <em>stereo</em> is to use corresponding points $p$ and $p&#39;$ to estimate the location of a 3D point by point $P$ using triangulation. But how do we knwow a point $p$ in one image actually corresponds to $p&#39;$ in another. I will talk about alternative techniques that work well in reconstructing the 3D structure.</p>
</blockquote>
<h1 id="Active-stereo-mitigating-the-correspondence-problem"><a href="#Active-stereo-mitigating-the-correspondence-problem" class="headerlink" title="Active stereo - mitigating the correspondence problem"></a>Active stereo - mitigating the correspondence problem</h1><p>The main idea is to replace one of the cameras with a device that interacts with the 3D environment, usually by projecting a pattern onto the object that is easily identifiable from the second camera. The new projector-camera pair defines the same epipolar geometry  that we introduced for camera pairs. The image plane of the replaced camera is replaced with a <em>projector virtual plane</em>.<br>As the figure shows:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig1.png" class="" title="alt text"><br>The projector projects a point $p$ in the virtual plane onto the target object, producing a point $P$ in 3D space. This point $P$ can should be observed in the second camera as point $p&#39;$. Since we know the color, intensity... of the projection, so we can easily discover the correspoind observation in the second camera $p&#39;$.</p>
<p>A common strategy in <em>active stereo</em> is to project a vertical stripe $s$ instead of a single point $p$. Like above figure shows, a vertical stripe $s$ in the projector virtual plane is colored red. After observing from the camare, it may become brown and can be easily discovered. If the projector and camera are parallel or rectified, then correspoind points can be easily found by intersectiong $s&#39;$ with the horizontal epipolar lines (I have covered in <em>epipolar geometry</em> section). From correspondences, we can easily reconstruct the all 3D points on the stripe $S$ using triangulation methods I have covered in the last section. If we swip the line $s$ across the scene and repeat the process, we can reconstruct the entire shape of all visible objects in the scene. </p>
<p>The <em>prerequisite</em> of the algorithm to work is that the projector and the camera should be calibrated. We can<br>first calibrate the camera using a calibration rig. Then, by projecting known stripes onto the calibration rig, and using the corresponding observations in the newly calibrated camera, we can set up constraints for estimating the projector intrinsic and extrinsic parameters.</p>
<p>One limitation of projecting a single stripe onto objects is that it is rather slow, as the projector needs to swipe across the entire object. Furthermore, this means that this method cannot capture deformations in real time. A natural extension is to instead attempt to reconstruct the object from projecting a single frame or image. The idea is to project a known pattern of different stripes to the entire visible of the object, instead of a single stripe. The colors of these stripes are designed in such a way that the stripes can be uniquely identified from the image. As the figure shows:</p>
<img src="/2025/09/03/Active-and-Volumetric-Stereo/fig2.png" class="" title="alt text">
<h1 id="Volumetric-stereo"><a href="#Volumetric-stereo" class="headerlink" title="Volumetric stereo"></a>Volumetric stereo</h1><p>This inverts the problem of using correspondences to find 3D structure: we assume that the 3D point we are trying to estimate is within some contained, known valume and we then project the hypothesized 3D point back into the calibrated cameras and validate whether these projections are <em>consistent</em> across the multiple views. Because these techniques of <em>volumetric stereo</em> assume that the points we want to recover are contained in a limited volume, so these techniques are mostly used for recovering the 3D models of specific objects as opposed to recovering models of a scene, which may be unbounded.A figure explains this:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig4.png" class="" title="alt text"></p>
<p>Based on the definition of what it means to &quot;consistent&quot; when we reproject a 3D point in the contained volume back into the multiple image views, we will briefly outline three major techniques:</p>
<ul>
<li><em>space carving</em></li>
<li><em>shadow carving</em></li>
<li><em>voxel coloring</em></li>
</ul>
<h2 id="Space-carving"><a href="#Space-carving" class="headerlink" title="Space carving"></a>Space carving</h2><p>This idea derived from that the <em>contours</em> of an object provide a rich source of geometric information about the object. We name the area in the image plane enclosed by the projection of the <em>contours</em> as <em>silhouette</em>. <em>Space carving</em> uses the <em>silhouettes</em> of the object from multiple views to enforce consistency.</p>
<p>Look at the picture:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig5.png" class="" title="alt text"><br>The image center and the object contour in the image plane form an enveloping surface, called <em>visual cone</em>. The object must reside in each of these visual cones and must lie in the intersection of all these <em>visual cones</em>, called <em>visual hull</em>. So we can use the <em>visual hull</em> to estimate the structure of the object.</p>
<p>In real practice, we first need to define a working volume that we know the object is contained within. For example, if our cameras encircle the object, then we can say the working volume is the entire interior of the sapce enclosed by the cameras. We then divide the working volume into samll units, known as <em>voxels</em> (like <em>pixels</em> which is the smallest element of a picture), thus forming what is known as a voxel grid.  We take each voxel in the voxel grid and project it into each of the views. If the voxel is not contained by the silhouette in a view, then it is discarded. Consequently, at the end of the space carving algorithm, we are left with the voxels that are contained within the visual hull:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig8.png" class="" title="alt text"></p>
<p><em>Limitations</em>:</p>
<ul>
<li><em>Time-costing</em>: As we reduce the size of each voxel, the number of voxels required by the grid increases cubically. Therefore, to get a finer reconstruction results in large increases in time.</li>
<li><em>Dependency on the number of views, the preciseness of the silhouette, and even the shape of the object we are trying to reconstruct</em>： for example, it is incapable of modeling certain concavities of an object:<img src="/2025/09/03/Active-and-Volumetric-Stereo/fig6.png" class="" title="alt text">
</li>
</ul>
<h2 id="Shadow-carving"><a href="#Shadow-carving" class="headerlink" title="Shadow carving"></a>Shadow carving</h2><p>This method is used to circumvent the concavity problem. ONe important cue for determining the 3D shape of an object is the presence of <strong>self-shadows</strong>. Selfshadows are the shadows that an object projects on itself. For the case of concave objects, an object will often cast self-shadows in the concave region.</p>
<p>The setup:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig9.png" class="" title="alt text"><br>the general setup of shadow carving is very similar to space carving. An object is placed in a turntable that is viewed by a calibrated camera. However, there is an array of lights in known positions around the camera whoes states can be appropriately turned on and off. These lights will be used to make the object cast self-shadows.</p>
<p>First,we use <em>space carving</em> to gain an initial voxel grid. Then in each view, we turn on and off each light in the array surrounding the camera. Each light will produce a different <em>self-shadow</em> on the object. Upon identifying the shadow in the image plane, we can then find the voxels on the surface of the grid that are in the visual cone of shadow. These surface voxels allow us to then make a new visual cone with the image source. We then leverage the useful fact that a voxel that is part of both visual cones cannot be part of the object to eliminate voxels in the concavity. The illustration:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig10.png" class="" title="alt text"></p>
<p>The <em>shadow carving</em> shares similar limitations with <em>space carving</em>: The runtime scales cubically with the resolution of the voxel grid. However, if there are $N $ lights, then shadow carving takes approximately $N + 1$ times longer than space carving, as each voxel needs to be projected into the camera and each of the $N$ lights.</p>
<h2 id="Voxel-coloring"><a href="#Voxel-coloring" class="headerlink" title="Voxel coloring"></a>Voxel coloring</h2><p>This method uses color consistency instead of contour consistency in space carving.</p>
<p>Given images from multiple views of an object that we want to reconstruct. For each voxel, we look at its corresponding projections in each of the images and compare the color of each of these projections. If the colors of these projections sufficiently match, then we mark the voxel as part of the object. One benefit of voxel coloring not present in space carving is that color associated with the projections can be transferred to the voxel, giving a colored reconstruction.</p>
<p>One drawback of vanilla voxel coloring is that it produces a solution that is not necessarily unique. Like this:<br><img src="/2025/09/03/Active-and-Volumetric-Stereo/fig11.png" class="" title="alt text"></p>
<p>In particular, we want to traverse the voxels layer by layer, starting with voxels closer to the cameras and then progress to further away voxels. When using this order, we perform the color consistency check. Then, we check if the voxel is viewable by at least two of the cameras, which constructs our visibility constraint. If the voxel was not viewable by at least two cameras, then it must be occluded and thus not part of the object. Notice that our order of processing the closer voxels allows us to make sure that we keep the voxels that can occlude later processed voxels to enforce this visibility<br>constraint.</p>

        </article>
        <section class="post-near">
            <ul>
                
                    <li>上一篇: <a href="/2025/09/03/3D-Representation/">3D Representation</a></li>
                
                
                    <li>下一篇: <a href="/2025/09/02/rlbook-reading-ch1-intro/">rlbook reading: ch1-intro</a></li>
                
            </ul>
        </section>
        
            <section class="post-tags">
            <a class="-none-link" href="/tags/3D-Vision/" rel="tag">3D Vision</a><a class="-none-link" href="/tags/computer-vision/" rel="tag">computer vision</a>
            </section>
        
    
        <section class="post-author">
        
            <figure class="author-avatar">
                <img src="https://sdn.geekzu.org/avatar/d22eb460ecab37fcd7205e6a3c55c228?s=200&r=X&d=" alt="Hingle" />
            </figure>
        
            <div class="author-info">
                <h4>Hingle</h4>
                <p>请在这里设置你的作者信息</p>
            </div>
        </section>
    
    </div>
</main>

    <footer>
    <div class="buttons">
        <button class="to-top" href="#"></button>
    </div>
    <div class="wrap min">
        <section class="widget">
            <div class="row">
                <div class="col-m-4">
                    <h3 class="title-recent">最新文章：</h3>
                    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/09/03/3D-Representation/">3D Representation</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/03/Active-and-Volumetric-Stereo/">Active and Volumetric Stereo</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/02/rlbook-reading-ch1-intro/">rlbook reading: ch1-intro</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/01/stereo-systems/">Stereo Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/08/30/epipolar-geometry/">epipolar-geometry</a></li></ul>
                </div>
                <div class="col-m-4">
                    <h3 class="title-date">时光机：</h3>
                    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/09/">九月 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/08/">八月 2025</a></li></ul>
                </div>
                <div class="col-m-4">
                    <h3 class="title-tags">标签云：</h3>
                    <a href="/tags/3D-Vision/" style="font-size: 15px;">3D Vision</a> <a href="/tags/computer-vision/" style="font-size: 20px;">computer vision</a> <a href="/tags/math/" style="font-size: 10px;">math</a> <a href="/tags/read-book/" style="font-size: 10px;">read book</a> <a href="/tags/rl/" style="font-size: 10px;">rl</a>
                </div>
            </div>
        </section>
        <section class="sub-footer">
            <p>© 2025 <a href="/">guochuan</a>. All Rights Reserved. Theme By <a href="https://github.com/Dreamer-Paul/Hingle" target="_blank" rel="nofollow">Hingle</a>.</p>
        </section>
    </div>
</footer>


<script src="/static/kico.js"></script>
<script src="/static/hingle.js"></script>
<script src="/static/todo.js"></script>



<script>var hingle = new Paul_Hingle({"copyright":true,"night":true});</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
